{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azhara001/cs182hw8/blob/main/Copy_of_tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CX7yDx924O9n",
        "outputId": "f0b28dcf-ce0e-4dcf-a70f-5b7e0177c715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-31 01:37:42--  https://raw.githubusercontent.com/Berkeley-CS182/cs182fa23_public/main/q_wandbai/architectures.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1618 (1.6K) [text/plain]\n",
            "Saving to: ‘architectures.py’\n",
            "\n",
            "\rarchitectures.py      0%[                    ]       0  --.-KB/s               \rarchitectures.py    100%[===================>]   1.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-31 01:37:42 (23.5 MB/s) - ‘architectures.py’ saved [1618/1618]\n",
            "\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=bfa9cf6b1cf2ada895010a878ceaa00523db6cefb9d9f27a94d0cd24b1d8707a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/Berkeley-CS182/cs182fa23_public/main/q_wandbai/architectures.py\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DWUT-gQ54O9o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from architectures import BasicConvNet, ResNet18, MLP\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvdQsiq4O9o"
      },
      "source": [
        "# Exploring Tensorboard\n",
        "Tensorboard is a local tool for visualizing images, metrics, histograms, and more. It is designed for tensorflow, but can be integrated with torch. Let's explore tensorboard usage with an example:\n",
        "\n",
        "```\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# To start a run, call the following\n",
        "writer = SummaryWriter(comment=f'Name_of_Run')\n",
        "\n",
        "# When you want to log a value, use the writer. When adding a scalar, the format is as follows:\n",
        "# add_scalar(tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n",
        "writer.add_scalar('Training Loss', loss.item(), step)\n",
        "\n",
        "# Finally, when you are done logging values, close the writer\n",
        "writer.close()\n",
        "```\n",
        "There are many other functionalities and methods that you are free to explore, but will not be mentioned in this notebook.\n",
        "\n",
        "## Your Task\n",
        "We will be once again building classifiers for the CIFAR-10. There are various architectures set up for you to use in the architectures.py file. Using tensorboard, please search through 5 different hyperparameter configurations. Examples of choices include: learning rate, batch size, architecture, optimization algorithm, etc. Please submit the generated plots on your pdf and answer question A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g_A9q6SE4O9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c0d050-408e-40d6-bed6-379d4bf05091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bce_Maeu4O9p",
        "outputId": "49a77b3e-af64-48c7-94de-2b195135d257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 45570788.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                    download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Hej_k6t-4O9p"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    'learning_rate' : [1e-2],#, 1e-2, 1e-1, 1, 1e1],\n",
        "    'optimizer:' : ['SGD', 'ADAM','AdamW']\n",
        "}\n",
        "\n",
        "class BasicConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UTG57mom4O9p"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "def run(lr=1e-3, batch_size=128):\n",
        "    # Perhaps you want to make a function to train on a certain set of hyperparameters\n",
        "    # Don't forget to use tensorboard\n",
        "    writer = SummaryWriter(comment=f'lr:{lr}_batch_size:{batch_size}')\n",
        "    model = BasicConvNet().to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "    train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True)\n",
        "    test_loader = DataLoader(testset,batch_size=batch_size,shuffle=True)\n",
        "    step=0\n",
        "    for _ in range(int(epochs*128/batch_size)):\n",
        "      dataiter = iter(test_loader)\n",
        "      for i, data in enumerate(train_loader):\n",
        "        X,Y = data\n",
        "        X,Y = X.to(device),Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X)\n",
        "        loss = loss_fn(output,Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        accuracy = torch.mean((torch.argmax(output, dim=1) == Y).float()).item() * 100\n",
        "\n",
        "        if i % log_freq == 0:\n",
        "              writer.add_scalar('Training Loss', loss.item(), step)\n",
        "              writer.add_scalar('Training Accuracy', accuracy, step)\n",
        "        if i % eval_freq == 0:\n",
        "            eval_inputs, eval_labels = next(dataiter)\n",
        "            eval_inputs, eval_labels = eval_inputs.to(device), eval_labels.to(device)\n",
        "            #eval_inputs = torch.flatten(eval_inputs, start_dim=1)\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                eval_outputs = model(eval_inputs)\n",
        "            eval_loss = loss_fn(eval_outputs, eval_labels)\n",
        "            model.train()\n",
        "            eval_accuracy = torch.mean((torch.argmax(eval_outputs, dim=1) == eval_labels).float()).item() * 100\n",
        "            writer.add_scalar('Validation Loss', eval_loss.item(), step)\n",
        "            writer.add_scalar('Validation Accuracy', eval_accuracy, step)\n",
        "        step += 1\n",
        "        print(f'\\rStep: {step}', end='')\n",
        "    print(f'\\nRun with (lr: {lr}, batch_size: {batch_size}) has finished')\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-3, 1e-4]\n",
        "batch_sizes = [128, 256]\n",
        "log_freq = 2\n",
        "eval_freq = 5\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        run(lr, batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQB3XZbxYPDL",
        "outputId": "bb31ea8a-aec3-401c-959c-33a661df0aaa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 782\n",
            "Run with (lr: 0.001, batch_size: 128) has finished\n",
            "Step: 196\n",
            "Run with (lr: 0.001, batch_size: 256) has finished\n",
            "Step: 782\n",
            "Run with (lr: 0.0001, batch_size: 128) has finished\n",
            "Step: 196\n",
            "Run with (lr: 0.0001, batch_size: 256) has finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26wkH0AlZAoi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TasYSNwfc90"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}